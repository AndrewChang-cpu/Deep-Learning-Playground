{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342c0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "# from torchvision.transforms import ToTensorfrom\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "669251ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file points shape (753876, 5)\n",
      "(205861, 5)\n",
      "(205861, 5)\n"
     ]
    }
   ],
   "source": [
    "class KDTreeDataset(Dataset):\n",
    "    def __init__(self, pts_file, split=0):\n",
    "        points = np.loadtxt(pts_file, delimiter=' ')\n",
    "        points = np.delete(points, -2, 1)\n",
    "        points = np.delete(points, -2, 1)\n",
    "        # points = np.delete(points, -2, 1)\n",
    "\n",
    "        if split == 1:\n",
    "            points = points[:len(points)//2]\n",
    "        elif split == 2:\n",
    "            points = points[len(points)//2:]\n",
    "\n",
    "        print('file points shape', points.shape)\n",
    "        \n",
    "        self.dim_scalars = []\n",
    "        for i in range(4):\n",
    "            dim_min, dim_max = min(points[:,i]), max(points[:,i])\n",
    "            points[:,i] = (points[:,i] - dim_min) / (dim_max - dim_min)\n",
    "            self.dim_scalars.append(dim_max - dim_min)\n",
    "        self.data = points\n",
    "\n",
    "        self.tree = KDTree(self.data[:, :3])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) // 15000\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Return batches of 25000 points\n",
    "        # print('Starting point', self.data[idx * 10000, :3])\n",
    "        noisy_point = self.data[idx * 15000, :3] + np.random.normal(0, 0.01, 3)[0]\n",
    "        # print('Noisy point', noisy_point)\n",
    "        \n",
    "        _, indicies = self.tree.query(noisy_point, k=25000)\n",
    "\n",
    "        xyzirn = self.data[indicies, :-1]  # x, y, z, ***intensity, return number, number of returns***\n",
    "        label = self.data[indicies, -1] == 5\n",
    "\n",
    "        xyzirn = torch.from_numpy(xyzirn.T).float() # intensity, z, y, x from top to bottom? or other way around\n",
    "        label = torch.tensor(label).long()\n",
    "        \n",
    "        # print(xyzirn.size(), label.size())\n",
    "\n",
    "        return xyzirn, label\n",
    "\n",
    "class NormalDataset(Dataset):\n",
    "    def __init__(self, pts_file, split=0):\n",
    "        points = np.loadtxt(pts_file, delimiter=' ')\n",
    "        points = np.delete(points, -2, 1)\n",
    "        points = np.delete(points, -2, 1)\n",
    "        # points = np.delete(points, -2, 1)\n",
    "\n",
    "        if split == 1:\n",
    "            points = points[:len(points)//2]\n",
    "        elif split == 2:\n",
    "            points = points[len(points)//2:]\n",
    "\n",
    "        print(points.shape)\n",
    "        # do i need to min max intensity, return number, number of returns, etc.? probably not\n",
    "        self.dim_scalars = []\n",
    "        for i in range(4):\n",
    "            dim_min, dim_max = min(points[:,i]), max(points[:,i])\n",
    "            points[:,i] = (points[:,i] - dim_min) / (dim_max - dim_min)\n",
    "            self.dim_scalars.append(dim_max - dim_min)\n",
    "        self.data = points\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) // 25000\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Return batches of 25000 points\n",
    "        xyzirn = self.data[idx * 25000: (idx + 1) * 25000, :-1]  # x, y, z, ***intensity, return number, number of returns***\n",
    "        label = self.data[idx * 25000: (idx + 1) * 25000, -1] == 5\n",
    "\n",
    "        xyzirn = torch.from_numpy(xyzirn.T).float() # intensity, z, y, x from top to bottom\n",
    "        label = torch.tensor(label).long()\n",
    "        \n",
    "        return xyzirn, label\n",
    "\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), r\"data\\Point Cloud\")\n",
    "training_data = KDTreeDataset(os.path.join(data_folder, r\"Traininig.pts\"))\n",
    "validation_data = NormalDataset(os.path.join(data_folder, r\"Testing.pts\"), split=1)\n",
    "testing_data = NormalDataset(os.path.join(data_folder, r\"Testing.pts\"), split=2)\n",
    "train_dataloader = DataLoader(training_data, batch_size=5, shuffle=True) # can/should i use shuffle, try lowering it?\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=5, shuffle=False)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=5, shuffle=False)\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc807f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97740441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 25000])\n",
      "torch.Size([5, 4, 25000])\n",
      "torch.Size([5, 4, 25000])\n",
      "torch.Size([5, 4, 25000])\n",
      "torch.Size([5, 4, 25000])\n",
      "torch.Size([5, 4, 25000])\n",
      "torch.Size([5, 4, 25000])\n",
      "torch.Size([5, 4, 25000])\n",
      "torch.Size([5, 4, 25000])\n",
      "torch.Size([5, 4, 25000])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_dataloader):\n",
    "    print(data[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d56a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "def plot_view(entire_view = True):\n",
    "    # Assuming pc is your point cloud data, in shape (N, 3)\n",
    "    pc_num = 2\n",
    "\n",
    "    if entire_view:\n",
    "        pc = training_data.data[:, :3]\n",
    "        labels = training_data.data[:, -1] == 5\n",
    "    else:\n",
    "        pc, labels = training_data[pc_num]\n",
    "        pc = pc.T.numpy()[:, :3]\n",
    "        labels = labels.numpy()\n",
    "\n",
    "    pc = copy.deepcopy(pc)\n",
    "    for i in range(3):\n",
    "        pc[:, i] *= training_data.dim_scalars[i]\n",
    "\n",
    "    print(Counter(labels), len(labels))\n",
    "\n",
    "    # Define colors for each label\n",
    "    color_map = {0: [0.5, 0.5, 0.5],  # Gray color for label 0\n",
    "                1: [1.0, 0.0, 0.0]}  # Red color for label 1\n",
    "\n",
    "    # Map each label to a color\n",
    "    colors = np.array([color_map[label] for label in labels])\n",
    "\n",
    "    # Create point cloud\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(pc)\n",
    "    point_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    # Visualize the point cloud\n",
    "    o3d.visualization.draw_geometries([point_cloud])\n",
    "# plot_view(entire_view = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66c21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch(data, labels):\n",
    "    pc, labels = data.transpose(1,2).reshape(-1, 4).numpy()[:, :3], labels.view(-1).numpy()\n",
    "    # print(labels.shape)\n",
    "    pc = copy.deepcopy(pc)\n",
    "    for i in range(3):\n",
    "        pc[:, i] *= training_data.dim_scalars[i]\n",
    "\n",
    "    print(Counter(list(labels)), len(labels))\n",
    "\n",
    "    # Define colors for each label\n",
    "    color_map = {0: [0.5, 0.5, 0.5],  # Gray color for label 0\n",
    "                1: [1.0, 0.0, 0.0]}  # Red color for label 1\n",
    "\n",
    "    # Map each label to a color\n",
    "    colors = np.array([color_map[label] for label in labels])\n",
    "\n",
    "    # Create point cloud\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(pc)\n",
    "    point_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    # Visualize the point cloud\n",
    "    o3d.visualization.draw_geometries([point_cloud])\n",
    "# for i, data in enumerate(train_dataloader):\n",
    "#     print(data[0].size())\n",
    "#     plot_batch(*data)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4086ae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "# device = (\n",
    "#     \"cuda\"\n",
    "#     if torch.cuda.is_available()\n",
    "#     else \"mps\"\n",
    "#     if torch.backends.mps.is_available()\n",
    "#     else \"cpu\"\n",
    "# )\n",
    "device = 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363013ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import  NamedTuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "class _KNN(NamedTuple):\n",
    "    neighbors: Tensor\n",
    "    dists: Tensor\n",
    "    indices: Tensor\n",
    "\n",
    "def gather_neighbors(x: Tensor, idx: Tensor):\n",
    "    \"\"\"\n",
    "    Gather K nearest neighbors into single tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor\n",
    "        Original data of shape [B, N, C].\n",
    "    idx : Tensor\n",
    "        Index positions gathered from topk of the pairwise distance matrix of shape\n",
    "        [B, N, K].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor\n",
    "        K nearest neighbors of shape [B, N, K, C].\n",
    "    \"\"\"\n",
    "    C = x.shape[-1]\n",
    "    K = idx.shape[-1]\n",
    "    idx_expanded = idx[:, :, :, None].expand(-1, -1, -1, C)  # B, N, K, C\n",
    "\n",
    "    return x[:, :, None].expand(-1, -1, K, -1).gather(1, idx_expanded)\n",
    "\n",
    "def knn(\n",
    "    x1: Tensor,\n",
    "    x2: Tensor,\n",
    "    k: int,\n",
    "    xyz_only: bool = True,\n",
    "    self_loop: bool = True,\n",
    "    topk: bool = False,\n",
    ") -> _KNN:\n",
    "    \"\"\"\n",
    "    Find the K nearest neighbors in y for every row in x.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x1 : Tensor\n",
    "        Origin input of shape (*, N, C).\n",
    "    x2 : Tensor\n",
    "        Target input of shape (*, M, C).\n",
    "    k : int\n",
    "        Number of K neighbors.\n",
    "    xyz_only : bool\n",
    "        Whether to use the first 3 channels, defaults to True.\n",
    "    self_loop : bool\n",
    "        Whether to include query point in nearest neighbors, defaults to True.\n",
    "    topk : bool\n",
    "        If true, calculate Largest K values returned from kernel, else return Smallest\n",
    "        K, defaults to False.\n",
    "    **kernel_kwargs : Any\n",
    "        Keyword arguments to pass to kernel function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    KNN\n",
    "        NamedTuple of (neighbors, dists, indices).\n",
    "    \"\"\"\n",
    "    # [B, N, N]\n",
    "    if xyz_only:\n",
    "        pdists = torch.cdist(x1[..., :3], x2[..., :3])\n",
    "    else:\n",
    "        pdists = torch.cdist(x1, x2)\n",
    "\n",
    "    if not self_loop:\n",
    "        fill_value = torch.inf if not topk else -torch.inf\n",
    "        new_diag = torch.full(tuple(pdists.size()[:2]), fill_value, device=x1.device)\n",
    "        pdists = pdists.diagonal_scatter(new_diag, dim1=1, dim2=2)\n",
    "\n",
    "    # [B, N, K]\n",
    "    dists, idxs = torch.topk(pdists, k=k, largest=topk, dim=-1)\n",
    "\n",
    "    # [B, N, K, C]\n",
    "    nearest_neighbors = gather_neighbors(x1, idxs)\n",
    "\n",
    "    return _KNN(nearest_neighbors, dists, idxs)\n",
    "\n",
    "class EdgeConv(nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1801.07829.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels: int\n",
    "        Number of input channels.\n",
    "    out_channels: int\n",
    "        Number of output channels.\n",
    "    k: int\n",
    "        Number of neighbors to consider, defaults to 20.\n",
    "    depth: int\n",
    "        Number of convolutions to apply.\n",
    "    xyz_only: bool\n",
    "        If True, only use the first 3 channels for KNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        k: int = 20,\n",
    "        xyz_only: bool = False,\n",
    "        norm: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.xyz_only = xyz_only\n",
    "        graph_dim = in_channels * 2\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(graph_dim, out_channels, bias=norm is False),\n",
    "            nn.LayerNorm(torch.Size((k, out_channels))) if norm else nn.Identity(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(out_channels, out_channels, bias=norm is False),\n",
    "            nn.LayerNorm(torch.Size((k, out_channels))) if norm else nn.Identity(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        graph = self._create_graph(x)\n",
    "        return self.mlp(graph).max(dim=-2)[0]\n",
    "\n",
    "    def _create_graph(self, x):\n",
    "        xj = knn(x, x, self.k, self.xyz_only).neighbors\n",
    "        xi = x.unsqueeze(-2).expand_as(xj)\n",
    "        return torch.cat((xj - xi, xi), dim=-1)\n",
    "    \n",
    "class DGCNNBase(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        k: int,\n",
    "        global_dim: int = 1024,\n",
    "        edge_conv_dim: int = 64,\n",
    "        depth: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.edge_convs = nn.ModuleDict()\n",
    "        for i in range(depth):\n",
    "            inc = in_channels if i == 0 else edge_conv_dim\n",
    "            xyz_only = i == 0\n",
    "            self.edge_convs[f\"graph_{i}\"] = EdgeConv(\n",
    "                in_channels=inc,\n",
    "                out_channels=edge_conv_dim,\n",
    "                xyz_only=xyz_only,\n",
    "                k=k,\n",
    "            )\n",
    "\n",
    "        self.global_feature = nn.Sequential(\n",
    "            nn.Linear(edge_conv_dim * depth, global_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Input: (B, C, N)\n",
    "        EdgeConv(s): (B, N, C) -> (B, N, edge_conv_dim)\n",
    "        Global: (B, N, edge_conv_dim * depth) -> (B, 1, global_dim)\n",
    "        \"\"\"\n",
    "        local_features = []\n",
    "        for edge_conv in self.edge_convs.values():\n",
    "            x = edge_conv(x)\n",
    "            local_features.append(x)\n",
    "        local_features = torch.cat(local_features, dim=-1)\n",
    "        global_features = self.global_feature(local_features).max(\n",
    "            dim=-2, keepdims=True\n",
    "        )[0]\n",
    "        return local_features, global_features\n",
    "    \n",
    "class DGCNNSegmentation(nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1801.07829\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Number of input channels.\n",
    "    k : Optional[int]\n",
    "        K neighbors for EdgeConv, defaults to 20.\n",
    "    global_dim : Optional[int]\n",
    "        Features to compute for global features, defaults to 1024.\n",
    "    edge_conv_dim : Optional[int]\n",
    "        EdgeConv dimension, defaults to 64.\n",
    "    depth : Optional[int]\n",
    "        Number of EdgeConv layers, defaults to 3.\n",
    "    out_channels : int\n",
    "        Number of output channels.\n",
    "    dropout : Optional[float]\n",
    "        Dropout percentage, defaults to 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        k=20,\n",
    "        global_dim=1024,\n",
    "        edge_conv_dim=64,\n",
    "        depth=3,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = DGCNNBase(\n",
    "            in_channels=in_channels,\n",
    "            k=k,\n",
    "            global_dim=global_dim,\n",
    "            edge_conv_dim=edge_conv_dim,\n",
    "            depth=depth,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(int(global_dim + (edge_conv_dim * depth)), 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        local_features, global_features = self.base(x)\n",
    "        global_features = global_features.repeat(1, x.shape[1], 1)\n",
    "        x = torch.cat([local_features, global_features], dim=-1)\n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfd973be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform_regularizer(trans):\n",
    "    d = trans.size(1)\n",
    "    I = torch.eye(d).unsqueeze(0).to(device)\n",
    "    loss = torch.linalg.norm(I - torch.bmm(trans, trans.transpose(2,1)), dim=(1,2))\n",
    "    loss = torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cad6a3",
   "metadata": {},
   "source": [
    "## Online Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294c68c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 25000, 4])\n",
      "Iteration 1 torch.Size([125000, 2]) torch.Size([125000])\n",
      "torch.Size([5, 25000, 4])\n",
      "Iteration 2 torch.Size([125000, 2]) torch.Size([125000])\n",
      "torch.Size([5, 25000, 4])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "classifier = DGCNNSegmentation(in_channels=4, out_channels=2).to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.0005, betas=(0.9, 0.999))\n",
    "\n",
    "num_batch = len(training_data)\n",
    "\n",
    "for epoch in range(120):\n",
    "    classifier.train()\n",
    "    train_loss, train_f1, train_acc = 0.0, 0.0, 0.0\n",
    "    predictions, labels = np.array([]), np.array([])\n",
    "    for i, data in enumerate(train_dataloader, 1):\n",
    "        points, target = data\n",
    "        points, target = points.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = classifier(points.permute(0,2,1))\n",
    "        pred = pred.view(-1, num_classes)\n",
    "        target = target.view(-1, 1).squeeze()\n",
    "        print(f'Iteration {i}', pred.size(), target.size())\n",
    "\n",
    "        loss = loss_func(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += float(loss) # JUST USING LOSS ACCUMULATES HISTORY\n",
    "\n",
    "        predictions = np.append(predictions, pred.max(1)[1].cpu())\n",
    "        labels = np.append(labels, target.cpu())\n",
    "\n",
    "    train_f1 = f1_score(predictions, labels)\n",
    "    train_acc = sum(predictions == labels)/float(len(labels))\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "\n",
    "    classifier.eval()\n",
    "    valid_loss, valid_f1, valid_acc = 0.0, 0.0, 0.0\n",
    "    predictions, labels = np.array([]), np.array([])\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validation_dataloader):\n",
    "            points, target = data\n",
    "            points, target = points.to(device), target.to(device)\n",
    "            pred,  trans_feat = classifier(points.permute(0,2,1))\n",
    "            pred = pred.view(-1, num_classes)\n",
    "            target = target.view(-1, 1).squeeze()\n",
    "\n",
    "            loss = loss_func(pred, target)\n",
    "            valid_loss += float(loss)\n",
    "\n",
    "            predictions = np.append(predictions, pred.max(1)[1].cpu())\n",
    "            labels = np.append(labels, target.cpu())\n",
    "\n",
    "    valid_f1 = f1_score(predictions, labels)\n",
    "    valid_acc = sum(predictions == labels)/float(len(labels))\n",
    "    valid_loss /= len(validation_dataloader)\n",
    "\n",
    "\n",
    "    writer.add_scalars('losses', {'training':train_loss, 'validation':valid_loss}, global_step=epoch)\n",
    "    writer.add_scalars('f1 scores', {'training':train_f1, 'validation':valid_f1}, global_step=epoch)\n",
    "\n",
    "    print(f'[{epoch}] train loss: {train_loss} accuracy: {train_acc} f1 score: {train_f1}')\n",
    "    print(f'[{epoch}] validation loss: {valid_loss} accuracy: {valid_acc} f1 score: {valid_f1}')\n",
    "    print()\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825faa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.3325457767277023\n",
      "F1 score:  0.34448641403679786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "outputs = None\n",
    "points = None\n",
    "x = y = None\n",
    "def test_PointNet(pointnet, test_dataloader, device):\n",
    "    global outputs, points, x, y\n",
    "    pointnet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (points, labels) in enumerate(test_dataloader):\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "            outputs, _ = pointnet(points)\n",
    "            _, predicted = torch.max(outputs.data, 2)\n",
    "            total += labels.size(0) * labels.size(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            x = labels.view(-1).to('cpu').numpy()\n",
    "            y = predicted.view(-1).to('cpu').numpy()\n",
    "            f1 = f1_score(x, y)\n",
    "            print('F1 score: ', f1)\n",
    "            plot_batch(points, predicted)\n",
    "            plot_batch(points, labels)\n",
    "    \n",
    "test_PointNet(classifier, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd5806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20070\n",
      "20965\n",
      "tensor([[[-0.0199, -3.9263],\n",
      "         [-0.0199, -3.9261],\n",
      "         [-0.0199, -3.9259],\n",
      "         ...,\n",
      "         [-0.3394, -1.2456],\n",
      "         [-0.1242, -2.1474],\n",
      "         [-0.1215, -2.1678]],\n",
      "\n",
      "        [[-0.1984, -1.7152],\n",
      "         [-0.1861, -1.7731],\n",
      "         [-0.1867, -1.7700],\n",
      "         ...,\n",
      "         [-0.0231, -3.7794],\n",
      "         [-0.0232, -3.7755],\n",
      "         [-0.0233, -3.7693]],\n",
      "\n",
      "        [[-0.0218, -3.8362],\n",
      "         [-0.0219, -3.8333],\n",
      "         [-0.0320, -3.4593],\n",
      "         ...,\n",
      "         [-1.0450, -0.4334],\n",
      "         [-1.0428, -0.4346],\n",
      "         [-1.0892, -0.4102]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(sum(x))\n",
    "print(sum(y))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe45c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels has 54930 zeroes and 20070 ones\n",
      "Predictions has 54035 zeroes and 20965 ones\n"
     ]
    }
   ],
   "source": [
    "print(f\"Labels has {sum(x == 0)} zeroes and {sum(x == 1)} ones\")\n",
    "print(f\"Predictions has {sum(y == 0)} zeroes and {sum(y == 1)} ones\")\n",
    "\n",
    "\n",
    "# writer.add_graph(model, points)\n",
    "# writer.flush()\n",
    "# writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07c64757",
   "metadata": {},
   "source": [
    "73.4029268292683% on roofs (5)\n",
    "86.77414634146342% on trees (8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
